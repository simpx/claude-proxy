{
  "description": "Basic user message conversion from Claude to OpenAI format",
  "claude_request": {
    "model": "claude-3-haiku-20240307",
    "max_tokens": 1000,
    "messages": [
      {
        "role": "user",
        "content": "Hello, how are you?"
      }
    ]
  },
  
  // -----------------------------------------------------------------------------
  // 期望输出：转换后的 OpenAI API 请求格式
  // -----------------------------------------------------------------------------
  "expected_openai_request": {
    "model": "gpt-4o-mini", // 根据模型映射: haiku -> small_model
    "messages": [
      {
        "role": "user",
        "content": "Hello, how are you?"
      }
    ],
    "max_tokens": 1000
  },
  
  // -----------------------------------------------------------------------------
  // 模拟：OpenAI API 响应 (用于测试反向转换)
  // -----------------------------------------------------------------------------
  "openai_response": {
    "id": "chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW",
    "object": "chat.completion",
    "created": 1677652288,
    "model": "gpt-4o-mini",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "Hello! I'm doing well, thank you for asking. How can I help you today?"
        },
        "finish_reason": "stop"
      }
    ],
    "usage": {
      "prompt_tokens": 13,
      "completion_tokens": 17,
      "total_tokens": 30
    }
  },
  
  // -----------------------------------------------------------------------------
  // 期望输出：转换后的 Claude API 响应格式
  // -----------------------------------------------------------------------------
  "expected_claude_response": {
    "id": "chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW",
    "type": "message",
    "role": "assistant",
    "model": "claude-3-haiku-20240307", // 保持原始请求的模型名
    "content": [
      {
        "type": "text",
        "text": "Hello! I'm doing well, thank you for asking. How can I help you today?"
      }
    ],
    "stop_reason": "end_turn", // finish_reason "stop" -> stop_reason "end_turn"
    "usage": {
      "input_tokens": 13,   // prompt_tokens -> input_tokens
      "output_tokens": 17   // completion_tokens -> output_tokens
    }
  }
}