"""
æ•°æ®é©±åŠ¨çš„è½¬æ¢æµ‹è¯•è¿è¡Œå™¨
åŠ è½½YAMLæµ‹è¯•æ¡ˆä¾‹æ–‡ä»¶å¹¶æ‰§è¡Œè½¬æ¢æµ‹è¯•
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
import json


@dataclass
class ConversionTestCase:
    """è½¬æ¢æµ‹è¯•æ¡ˆä¾‹æ•°æ®ç±»"""
    file_name: str
    description: str
    category: str
    tags: List[str]
    claude_request: Optional[Dict[str, Any]]
    expected_openai_request: Optional[Dict[str, Any]]
    openai_response: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None
    expected_claude_response: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None
    test_config: Optional[Dict[str, bool]] = None
    env: Optional[Dict[str, str]] = None
    file_path: str = ""


class ConversionCaseLoader:
    """æµ‹è¯•æ¡ˆä¾‹åŠ è½½å™¨"""
    
    def __init__(self, cases_dir: str = "tests/unit/convert_cases"):
        self.cases_dir = Path(cases_dir)
        self.cases: List[ConversionTestCase] = []
    
    def load_all_cases(self) -> List[ConversionTestCase]:
        """åŠ è½½æ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹"""
        self.cases.clear()
        
        if not self.cases_dir.exists():
            raise FileNotFoundError(f"Cases directory not found: {self.cases_dir}")
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰JSONCæ–‡ä»¶
        jsonc_files = list(self.cases_dir.rglob("*.jsonc"))
        
        for jsonc_file in jsonc_files:
            try:
                case = self._load_case_file(jsonc_file)
                if case:
                    self.cases.append(case)
            except Exception as e:
                print(f"âŒ Failed to load case file {jsonc_file}: {e}")
        
        print(f"âœ… Loaded {len(self.cases)} conversion test cases")
        return self.cases
    
    def _load_case_file(self, file_path: Path) -> Optional[ConversionTestCase]:
        """åŠ è½½å•ä¸ªæ¡ˆä¾‹æ–‡ä»¶ (æ”¯æŒJSONCæ ¼å¼)"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç§»é™¤JSONCæ³¨é‡Š
            content = self._remove_jsonc_comments(content)
            data = json.loads(content)
            
            # éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªå¿…éœ€å­—æ®µç»„åˆ
            has_request_test = 'claude_request' in data and 'expected_openai_request' in data
            has_response_test = 'openai_response' in data and 'expected_claude_response' in data
            
            if not (has_request_test or has_response_test):
                raise ValueError(f"Must have either (claude_request + expected_openai_request) or (openai_response + expected_claude_response)")
            
            # æ¨æ–­category from file path
            category = file_path.parent.name
            file_name = file_path.stem  # æ–‡ä»¶åä¸å¸¦æ‰©å±•å
            
            # å¤„ç†modelå­—æ®µï¼šå½“requestå’Œexpectedéƒ½ä¸å«modelæ—¶ï¼Œç»™å®ƒä»¬æ·»åŠ é»˜è®¤å€¼
            claude_request = data.get('claude_request')
            expected_openai_request = data.get('expected_openai_request')
            openai_response = data.get('openai_response')
            expected_claude_response = data.get('expected_claude_response')
            
            # å¤„ç†è¯·æ±‚è½¬æ¢çš„modelå­—æ®µ
            should_skip_model_mapping = False
            if claude_request and expected_openai_request:
                if 'model' not in claude_request and 'model' not in expected_openai_request:
                    # ä¸¤ä¸ªéƒ½æ²¡æœ‰modelï¼Œæ·»åŠ é»˜è®¤å€¼ï¼Œå¹¶æ ‡è®°ä¸æµ‹è¯•modelæ˜ å°„
                    claude_request = claude_request.copy()
                    expected_openai_request = expected_openai_request.copy()
                    claude_request['model'] = 'claude-3-haiku-20240307'
                    expected_openai_request['model'] = 'gpt-4o-mini'
                    should_skip_model_mapping = True
            
            # å¤„ç†å“åº”è½¬æ¢çš„modelå­—æ®µï¼ˆè·³è¿‡streamingå“åº”çš„listæ ¼å¼ï¼‰
            if (openai_response and expected_claude_response 
                and not isinstance(openai_response, list) 
                and not isinstance(expected_claude_response, list)):
                if 'model' not in openai_response and 'model' not in expected_claude_response:
                    # ä¸¤ä¸ªéƒ½æ²¡æœ‰modelï¼Œæ·»åŠ é»˜è®¤å€¼
                    openai_response = openai_response.copy()
                    expected_claude_response = expected_claude_response.copy()
                    openai_response['model'] = 'gpt-4o-mini'
                    expected_claude_response['model'] = 'claude-3-haiku-20240307'
                    should_skip_model_mapping = True
            
            # æ›´æ–°test_config
            test_config = data.get('test_config', {}).copy()
            if should_skip_model_mapping:
                test_config['test_model_mapping'] = False
            
            return ConversionTestCase(
                file_name=file_name,
                description=data.get('description', ''),
                category=category,
                tags=data.get('tags', []),
                claude_request=claude_request,
                expected_openai_request=expected_openai_request,
                openai_response=openai_response,
                expected_claude_response=expected_claude_response,
                test_config=test_config,
                env=data.get('env', {}),
                file_path=str(file_path)
            )
        except Exception as e:
            print(f"âŒ Error loading {file_path}: {e}")
            return None
    
    def _remove_jsonc_comments(self, content: str) -> str:
        """ç§»é™¤JSONCä¸­çš„æ³¨é‡Š"""
        # ç§»é™¤å•è¡Œæ³¨é‡Š // ...
        content = re.sub(r'//.*$', '', content, flags=re.MULTILINE)
        
        # ç§»é™¤å¤šè¡Œæ³¨é‡Š /* ... */
        content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
        
        return content
    
    def get_cases_by_category(self, category: str) -> List[ConversionTestCase]:
        """æŒ‰ç±»åˆ«ç­›é€‰æ¡ˆä¾‹"""
        return [case for case in self.cases if case.category == category]
    
    def get_cases_by_tag(self, tag: str) -> List[ConversionTestCase]:
        """æŒ‰æ ‡ç­¾ç­›é€‰æ¡ˆä¾‹"""
        return [case for case in self.cases if tag in case.tags]


class ConversionTestValidator:
    """è½¬æ¢æµ‹è¯•éªŒè¯å™¨"""
    
    @staticmethod
    def validate_request_conversion(
        claude_request: Dict[str, Any],
        actual_openai_request: Dict[str, Any], 
        expected_openai_request: Dict[str, Any]
    ) -> Tuple[bool, List[str]]:
        """éªŒè¯è¯·æ±‚è½¬æ¢æ˜¯å¦æ­£ç¡®"""
        errors = []
        
        # éªŒè¯æ¨¡å‹æ˜ å°„
        if actual_openai_request.get('model') != expected_openai_request.get('model'):
            errors.append(f"Model mismatch: expected {expected_openai_request.get('model')}, got {actual_openai_request.get('model')}")
        
        # éªŒè¯æ¶ˆæ¯æ•°ç»„
        actual_messages = actual_openai_request.get('messages', [])
        expected_messages = expected_openai_request.get('messages', [])
        
        if len(actual_messages) != len(expected_messages):
            errors.append(f"Messages count mismatch: expected {len(expected_messages)}, got {len(actual_messages)}")
        else:
            for i, (actual_msg, expected_msg) in enumerate(zip(actual_messages, expected_messages)):
                if actual_msg != expected_msg:
                    errors.append(f"Message {i} mismatch: expected {expected_msg}, got {actual_msg}")
        
        # éªŒè¯å…¶ä»–å‚æ•°
        for key in ['max_tokens', 'stream']:
            if key in expected_openai_request:
                if actual_openai_request.get(key) != expected_openai_request.get(key):
                    errors.append(f"{key} mismatch: expected {expected_openai_request.get(key)}, got {actual_openai_request.get(key)}")
        
        return len(errors) == 0, errors
    
    @staticmethod
    def validate_response_conversion(
        openai_response: Dict[str, Any],
        actual_claude_response: Dict[str, Any],
        expected_claude_response: Dict[str, Any]
    ) -> Tuple[bool, List[str]]:
        """éªŒè¯å“åº”è½¬æ¢æ˜¯å¦æ­£ç¡®"""
        errors = []
        
        # éªŒè¯åŸºæœ¬å­—æ®µ
        basic_fields = ['id', 'type', 'role', 'model', 'stop_reason']
        for field in basic_fields:
            if field in expected_claude_response:
                if actual_claude_response.get(field) != expected_claude_response.get(field):
                    errors.append(f"{field} mismatch: expected {expected_claude_response.get(field)}, got {actual_claude_response.get(field)}")
        
        # éªŒè¯å†…å®¹è½¬æ¢
        actual_content = actual_claude_response.get('content', [])
        expected_content = expected_claude_response.get('content', [])
        
        if len(actual_content) != len(expected_content):
            errors.append(f"Content blocks count mismatch: expected {len(expected_content)}, got {len(actual_content)}")
        else:
            # éªŒè¯æ¯ä¸ªå†…å®¹å—çš„è¯¦ç»†å†…å®¹
            for i, (actual_block, expected_block) in enumerate(zip(actual_content, expected_content)):
                if actual_block.get('type') != expected_block.get('type'):
                    errors.append(f"Content block {i} type mismatch: expected {expected_block.get('type')}, got {actual_block.get('type')}")
                
                if actual_block.get('type') == 'text':
                    if actual_block.get('text') != expected_block.get('text'):
                        errors.append(f"Content block {i} text mismatch: expected '{expected_block.get('text')}', got '{actual_block.get('text')}'")
        
        # éªŒè¯ä½¿ç”¨ç»Ÿè®¡è½¬æ¢
        actual_usage = actual_claude_response.get('usage', {})
        expected_usage = expected_claude_response.get('usage', {})
        
        for key in ['input_tokens', 'output_tokens']:
            if key in expected_usage:
                if actual_usage.get(key) != expected_usage.get(key):
                    errors.append(f"Usage {key} mismatch: expected {expected_usage.get(key)}, got {actual_usage.get(key)}")
        
        return len(errors) == 0, errors
    
    @staticmethod
    def validate_model_mapping(claude_model: str, actual_openai_model: str, expected_openai_model: str) -> Tuple[bool, List[str]]:
        """éªŒè¯æ¨¡å‹æ˜ å°„æ˜¯å¦æ­£ç¡®"""
        if actual_openai_model != expected_openai_model:
            return False, [f"Model mapping incorrect: {claude_model} should map to {expected_openai_model}, got {actual_openai_model}"]
        return True, []


def print_case_summary(cases: List[ConversionTestCase]):
    """æ‰“å°æ¡ˆä¾‹æ±‡æ€»ä¿¡æ¯"""
    print(f"\nğŸ“Š Test Cases Summary:")
    print(f"Total cases: {len(cases)}")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    categories = {}
    for case in cases:
        categories[case.category] = categories.get(case.category, 0) + 1
    
    print("Categories:")
    for category, count in categories.items():
        print(f"  - {category}: {count} cases")
    
    # æŒ‰æ ‡ç­¾ç»Ÿè®¡
    all_tags = set()
    for case in cases:
        all_tags.update(case.tags)
    
    print(f"Tags: {', '.join(sorted(all_tags))}")


if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    loader = ConversionCaseLoader()
    cases = loader.load_all_cases()
    print_case_summary(cases)
    
    # æ‰“å°æ¡ˆä¾‹è¯¦æƒ…
    for case in cases:
        print(f"\nğŸ“ {case.file_name}")
        print(f"   Category: {case.category}")
        print(f"   Description: {case.description}")
        print(f"   Tags: {', '.join(case.tags)}")
        print(f"   File: {case.file_path}")