"""
æ•°æ®é©±åŠ¨çš„æ ¼å¼è½¬æ¢æµ‹è¯•
ä½¿ç”¨JSONCæ¡ˆä¾‹æ–‡ä»¶æµ‹è¯•Claudeåˆ°OpenAIçš„APIæ ¼å¼è½¬æ¢
"""

import pytest
import sys
import os
from pathlib import Path
from unittest.mock import patch
import asyncio

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.claude_proxy.providers.openai import OpenAIProvider
from src.claude_proxy.models.claude import ClaudeMessagesRequest
from .conversion_runner import ConversionCaseLoader, ConversionTestValidator


class TestConvertCases:
    """åŸºäºæ•°æ®é©±åŠ¨çš„æ ¼å¼è½¬æ¢æµ‹è¯•"""
    
    # é¢„è®¾çš„æµ‹è¯•ç¯å¢ƒå˜é‡
    TEST_ENV_VARS = {
        'OPENAI_API_KEY': 'sk-test-key-12345',
        'OPENAI_BASE_URL': 'https://api.openai.com/v1',
        'CLAUDE_PROXY_BIG_MODEL': 'gpt-4o',
        'CLAUDE_PROXY_SMALL_MODEL': 'gpt-4o-mini',
        'CLAUDE_PROXY_LOG_LEVEL': 'INFO',
        'CLAUDE_PROXY_HOST': '0.0.0.0',
        'CLAUDE_PROXY_PORT': '8080',
    }
    
    @classmethod
    def setup_class(cls):
        """æµ‹è¯•ç±»åˆå§‹åŒ–"""
        cls.loader = ConversionCaseLoader()
        cls.validator = ConversionTestValidator()
        cls.cases = cls.loader.load_all_cases()
        
        print(f"\nğŸ§ª Loaded {len(cls.cases)} conversion test cases")
    
    @pytest.mark.parametrize("case", [
        pytest.param(case, id=f"{case.category}::{case.file_name}") 
        for case in ConversionCaseLoader().load_all_cases()
        if (case.test_config.get('test_request_conversion', True) 
            and case.claude_request 
            and case.expected_openai_request)
    ])
    def test_request_conversion(self, case):
        """æµ‹è¯•Claudeè¯·æ±‚åˆ°OpenAIè¯·æ±‚çš„è½¬æ¢"""
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡ï¼Œå¯è¢«case.envè¦†ç›–
        test_env = self.TEST_ENV_VARS.copy()
        if hasattr(case, 'env') and case.env:
            test_env.update(case.env)
        
        with patch.dict(os.environ, test_env, clear=False):
            # æ¸…é™¤ç¼“å­˜çš„è®¾ç½®å¹¶é‡æ–°åŠ è½½é…ç½®
            import src.claude_proxy.config as config_module
            config_module._settings = None
            
            # åˆ›å»ºOpenAI providerå®ä¾‹
            provider = OpenAIProvider(
                api_key="test-key",
                base_url="https://api.openai.com/v1",
                timeout=30
            )
            
            # å°†Claudeè¯·æ±‚è½¬æ¢ä¸ºPydanticæ¨¡å‹
            claude_request = ClaudeMessagesRequest(**case.claude_request)
            
            # æ‰§è¡Œè½¬æ¢
            actual_openai_request = provider.convert_request(claude_request)
            
            # éªŒè¯è½¬æ¢ç»“æœ
            is_valid, errors = self.validator.validate_request_conversion(
                case.claude_request,
                actual_openai_request,
                case.expected_openai_request
            )
            
            # æ–­è¨€éªŒè¯ç»“æœ
            if not is_valid:
                error_msg = f"Request conversion failed for case '{case.file_name}':\n"
                for error in errors:
                    error_msg += f"  - {error}\n"
                error_msg += f"\nExpected: {case.expected_openai_request}\n"
                error_msg += f"Actual: {actual_openai_request}\n"
                error_msg += f"Case file: {case.file_path}"
                
                pytest.fail(error_msg)
    
    @pytest.mark.parametrize("case", [
        pytest.param(case, id=f"{case.category}::{case.file_name}")
        for case in ConversionCaseLoader().load_all_cases()
        if (case.test_config.get('test_response_conversion', True) 
            and case.openai_response 
            and case.expected_claude_response
            and not isinstance(case.openai_response, list))  # Exclude streaming (list format)
    ])
    def test_response_conversion(self, case):
        """æµ‹è¯•OpenAIå“åº”åˆ°Claudeå“åº”çš„è½¬æ¢"""
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡ï¼Œå¯è¢«case.envè¦†ç›–
        test_env = self.TEST_ENV_VARS.copy()
        if hasattr(case, 'env') and case.env:
            test_env.update(case.env)
        
        with patch.dict(os.environ, test_env, clear=False):
            # æ¸…é™¤ç¼“å­˜çš„è®¾ç½®å¹¶é‡æ–°åŠ è½½é…ç½®
            import src.claude_proxy.config as config_module
            config_module._settings = None
            
            # åˆ›å»ºOpenAI providerå®ä¾‹
            provider = OpenAIProvider(
                api_key="test-key",
                base_url="https://api.openai.com/v1",
                timeout=30
            )
            
            # åˆ›å»ºåŸå§‹Claudeè¯·æ±‚ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            claude_request = None
            if case.claude_request:
                claude_request = ClaudeMessagesRequest(**case.claude_request)
            
            # æ‰§è¡Œå“åº”è½¬æ¢
            actual_claude_response = provider.convert_response(
                case.openai_response,
                claude_request
            )
        
        # éªŒè¯è½¬æ¢ç»“æœ
        is_valid, errors = self.validator.validate_response_conversion(
            case.openai_response,
            actual_claude_response.model_dump(),
            case.expected_claude_response
        )
        
        # æ–­è¨€éªŒè¯ç»“æœ
        if not is_valid:
            error_msg = f"Response conversion failed for case '{case.file_name}':\n"
            for error in errors:
                error_msg += f"  - {error}\n"
            error_msg += f"\nExpected: {case.expected_claude_response}\n"
            error_msg += f"Actual: {actual_claude_response.model_dump()}\n"
            error_msg += f"Case file: {case.file_path}"
            
            pytest.fail(error_msg)
    
    @pytest.mark.parametrize("case", [
        pytest.param(case, id=f"{case.category}::{case.file_name}")
        for case in ConversionCaseLoader().load_all_cases()
        if (case.test_config.get('test_model_mapping', True) 
            and case.claude_request 
            and case.expected_openai_request
            and 'model' in case.claude_request
            and 'model' in case.expected_openai_request)
    ])
    def test_model_mapping(self, case):
        """æµ‹è¯•æ¨¡å‹æ˜ å°„æ˜¯å¦æ­£ç¡®"""
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡ï¼Œå¯è¢«case.envè¦†ç›–
        test_env = self.TEST_ENV_VARS.copy()
        if hasattr(case, 'env') and case.env:
            test_env.update(case.env)
        
        with patch.dict(os.environ, test_env, clear=False):
            # æ¸…é™¤ç¼“å­˜çš„è®¾ç½®å¹¶é‡æ–°åŠ è½½é…ç½®
            import src.claude_proxy.config as config_module
            config_module._settings = None
            
            from src.claude_proxy.config import map_claude_model
            
            claude_model = case.claude_request['model']
            expected_openai_model = case.expected_openai_request['model']
            actual_mapped_model = map_claude_model(claude_model)
        
        # éªŒè¯æ¨¡å‹æ˜ å°„
        is_valid, errors = self.validator.validate_model_mapping(
            claude_model,
            actual_mapped_model, 
            expected_openai_model
        )
        
        if not is_valid:
            error_msg = f"Model mapping failed for case '{case.file_name}':\n"
            for error in errors:
                error_msg += f"  - {error}\n"
            error_msg += f"Case file: {case.file_path}"
            
            pytest.fail(error_msg)
    
    @pytest.mark.parametrize("case", [
        pytest.param(case, id=f"{case.category}::{case.file_name}")
        for case in ConversionCaseLoader().load_all_cases()
        if (case.openai_response 
            and case.expected_claude_response
            and isinstance(case.openai_response, list)  # Streaming is indicated by list format
            and isinstance(case.expected_claude_response, list))
    ])
    @pytest.mark.asyncio
    async def test_streaming_conversion(self, case):
        """æµ‹è¯•æµå¼å“åº”è½¬æ¢"""
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡ï¼Œå¯è¢«case.envè¦†ç›–
        test_env = self.TEST_ENV_VARS.copy()
        if hasattr(case, 'env') and case.env:
            test_env.update(case.env)
        
        with patch.dict(os.environ, test_env, clear=False):
            import src.claude_proxy.config as config_module
            config_module._settings = None
            
            mock_client = self._create_mock_streaming_client(case.openai_response)
            
            provider = OpenAIProvider(
                api_key="test-key",
                base_url="https://api.openai.com/v1", 
                timeout=30,
                client=mock_client
            )
            
            from src.claude_proxy.models.claude import ClaudeMessagesRequest
            if case.claude_request:
                claude_request_obj = ClaudeMessagesRequest(**case.claude_request)
            else:
                claude_request_obj = ClaudeMessagesRequest(
                    model="claude-3-haiku-20240307",
                    max_tokens=100,
                    messages=[{"role": "user", "content": "test"}]
                )
            
            actual_events = []
            async for sse_event in provider.stream_complete(claude_request_obj, "test-request-id"):
                event = self._parse_sse_event(sse_event)
                if event:
                    actual_events.append(event)
            
            self._validate_streaming_events(
                actual_events,
                case.expected_claude_response, 
                case.file_name,
                case.file_path
            )
    
    def _create_mock_streaming_client(self, openai_chunks):
        """åˆ›å»ºæ¨¡æ‹Ÿæµå¼HTTPå®¢æˆ·ç«¯"""
        import json
        from unittest.mock import AsyncMock, MagicMock
        
        sse_lines = []
        for chunk in openai_chunks:
            sse_lines.append(f"data: {json.dumps(chunk)}")
        sse_lines.append("data: [DONE]")
        
        mock_response = MagicMock()
        mock_response.status_code = 200
        mock_response.raise_for_status = MagicMock()
        
        async def mock_aiter_lines():
            for line in sse_lines:
                yield line
        
        mock_response.aiter_lines = mock_aiter_lines
        
        class MockStreamContext:
            def __init__(self, response):
                self.response = response
            
            async def __aenter__(self):
                return self.response
            
            async def __aexit__(self, _exc_type, _exc_val, _exc_tb):
                return None
        
        mock_client = AsyncMock()
        
        def mock_stream(*_args, **_kwargs):
            return MockStreamContext(mock_response)
        
        mock_client.stream = mock_stream
        
        return mock_client
    
    def _parse_sse_event(self, sse_event_str):
        """è§£æproviderè¿”å›çš„SSEäº‹ä»¶å­—ç¬¦ä¸²ä¸ºJSONå¯¹è±¡"""
        import json
        
        if not sse_event_str.strip():
            return None
        
        lines = sse_event_str.strip().split('\n')
        data_line = None
        
        for line in lines:
            if line.startswith('data: '):
                data_line = line[6:]
                break
        
        if data_line:
            try:
                return json.loads(data_line)
            except json.JSONDecodeError:
                return None
        
        return None
    
    def _validate_streaming_events(self, actual_events, expected_events, case_name, case_file):
        """éªŒè¯æµå¼äº‹ä»¶åºåˆ—"""
        if len(actual_events) != len(expected_events):
            pytest.fail(f"Streaming event count mismatch in '{case_name}': expected {len(expected_events)}, got {len(actual_events)}\nCase file: {case_file}")
        
        for i, (actual, expected) in enumerate(zip(actual_events, expected_events)):
            if actual.get("type") != expected.get("type"):
                pytest.fail(f"Event {i} type mismatch in '{case_name}': expected {expected.get('type')}, got {actual.get('type')}\nCase file: {case_file}")
            
            if actual["type"] == "content_block_start":
                actual_block = actual.get("content_block", {})
                expected_block = expected.get("content_block", {})
                
                if actual_block.get("type") != expected_block.get("type"):
                    pytest.fail(f"Event {i} content_block type mismatch in '{case_name}': expected {expected_block.get('type')}, got {actual_block.get('type')}\nCase file: {case_file}")
                
                if actual_block.get("text") != expected_block.get("text"):
                    pytest.fail(f"Event {i} content_block text mismatch in '{case_name}': expected '{expected_block.get('text')}', got '{actual_block.get('text')}'\nCase file: {case_file}")
            
            elif actual["type"] == "content_block_delta":
                actual_text = actual.get("delta", {}).get("text", "")
                expected_text = expected.get("delta", {}).get("text", "")
                if actual_text != expected_text:
                    pytest.fail(f"Event {i} delta text mismatch in '{case_name}': expected '{expected_text}', got '{actual_text}'\nCase file: {case_file}")
            
            elif actual["type"] == "message_delta":
                actual_stop_reason = actual.get("delta", {}).get("stop_reason")
                expected_stop_reason = expected.get("delta", {}).get("stop_reason") 
                if actual_stop_reason != expected_stop_reason:
                    pytest.fail(f"Event {i} stop_reason mismatch in '{case_name}': expected '{expected_stop_reason}', got '{actual_stop_reason}'\nCase file: {case_file}")
                
                actual_usage = actual.get("usage", {})
                expected_usage = expected.get("usage", {})
                if expected_usage.get("output_tokens") is not None:
                    actual_tokens = actual_usage.get("output_tokens")
                    expected_tokens = expected_usage.get("output_tokens")
                    if actual_tokens != expected_tokens:
                        pytest.fail(f"Event {i} output_tokens mismatch in '{case_name}': expected {expected_tokens}, got {actual_tokens}\nCase file: {case_file}")
            
            elif actual["type"] == "message_start":
                actual_msg = actual.get("message", {})
                expected_msg = expected.get("message", {})
                
                if expected_msg.get("model") and actual_msg.get("model") != expected_msg.get("model"):
                    pytest.fail(f"Event {i} model mismatch in '{case_name}': expected {expected_msg.get('model')}, got {actual_msg.get('model')}\nCase file: {case_file}")
                
                if actual_msg.get("role") != expected_msg.get("role"):
                    pytest.fail(f"Event {i} role mismatch in '{case_name}': expected {expected_msg.get('role')}, got {actual_msg.get('role')}\nCase file: {case_file}")
    
    @classmethod
    def teardown_class(cls):
        """æµ‹è¯•ç±»æ¸…ç†"""
        print(f"\nâœ… Completed testing {len(cls.cases)} conversion cases")


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"])